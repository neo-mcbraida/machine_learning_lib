# Greedy Backpropagation Framework (C++ Deep Learning)

A custom deep learning framework written in C++ to explore the mathematical mechanics of backpropagation and greedy layer-wise training.  
This project was designed as a research and learning tool to gain a deeper, implementation-level understanding of gradient-based optimisation, error propagation, and network convergence dynamics.

---

## Key Features

- From-scratch neural network framework in modern C++ (C++17)
- Implements forward and backward propagation using analytical gradients
- Supports greedy layer-wise training, enabling isolated analysis of layer convergence
- Modular design for dense layers, activation functions, and loss functions
- Matrix-based computation engine (custom or Eigen-compatible)
- Lightweight, header-only core suitable for experimentation
- Provides clear visualisation hooks for gradient and loss inspection (optional)
